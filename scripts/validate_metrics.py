#!/usr/bin/env python3
"""
Validation script for CI pipeline outputs.
Verifies that metrics generated by Pharo/Moose are correct by:
  Level 1: Structural validation of the CSV
  Level 3: Logical invariants on metric values
  Level 2+4: Cross-verification — independently compute metrics from
             the Famix JSON and compare with the Pharo-generated CSV
"""

import sys
import json
import csv
import os

# ============================================================
# Configuration
# ============================================================
EXPECTED_COLUMNS = ['Nom_Classe', 'Nb_Methodes', 'Nb_Attributs', 'Lignes_de_Code']
SEPARATOR = ';'

# Tolerance for LOC comparison (Pharo reads source files, Python uses
# source anchor positions — minor differences are possible due to
# line ending handling)
LOC_TOLERANCE = 3


def load_csv(csv_path):
    """Load the Pharo-generated CSV."""
    rows = []
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f, delimiter=SEPARATOR)
        for row in reader:
            rows.append(row)
    return rows


def load_famix_json(json_path):
    """Load the Famix JSON model."""
    with open(json_path, 'r', encoding='utf-8') as f:
        return json.load(f)


def compute_metrics_from_json(data, source_root='.'):
    """
    Independently compute class metrics from the Famix JSON.
    This mirrors what Moose/Pharo does internally.
    """
    entities = {e['id']: e for e in data if 'id' in e}

    classes = [e for e in data if e.get('FM3') == 'FamixTypeScript.Class']
    methods = [e for e in data if e.get('FM3') == 'FamixTypeScript.Method']
    properties = [e for e in data if e.get('FM3') == 'FamixTypeScript.Property']

    metrics = {}
    for c in classes:
        name = c['name']

        # Count methods belonging to this class
        class_methods = [m for m in methods
                         if m.get('parentType', {}).get('ref') == c['id']]
        nb_methods = len(class_methods)

        # Count attributes belonging to this class
        class_attrs = [p for p in properties
                       if p.get('parentType', {}).get('ref') == c['id']]
        nb_attrs = len(class_attrs)

        # Compute LOC from source anchor (primary method)
        loc = 0
        sa_ref = c.get('sourceAnchor', {}).get('ref')
        if sa_ref and sa_ref in entities:
            sa = entities[sa_ref]
            fname = sa.get('fileName', '')
            start = sa.get('startPos', 0)
            end = sa.get('endPos', 0)
            filepath = os.path.join(source_root, fname) if not os.path.isabs(fname) else fname
            if os.path.exists(filepath):
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        content = f.read()
                    snippet = content[start - 1:end]
                    loc = len(snippet.splitlines())
                except Exception:
                    pass

        # Fallback: sum method LOC
        if loc == 0:
            for m in class_methods:
                loc += m.get('numberOfLinesOfCode', 0)

        metrics[name] = {
            'Nb_Methodes': nb_methods,
            'Nb_Attributs': nb_attrs,
            'Lignes_de_Code': loc
        }

    return metrics


# ============================================================
# Level 1: Structural validation
# ============================================================
def validate_structure(csv_rows, csv_path):
    errors = []

    if len(csv_rows) == 0:
        errors.append(f'CSV is empty: {csv_path}')
        return errors

    # Check columns
    actual_cols = list(csv_rows[0].keys())
    if actual_cols != EXPECTED_COLUMNS:
        errors.append(f'Column mismatch. Expected {EXPECTED_COLUMNS}, got {actual_cols}')

    # Check no empty values
    for i, row in enumerate(csv_rows):
        for col in EXPECTED_COLUMNS:
            val = row.get(col, '')
            if val is None or str(val).strip() == '':
                errors.append(f'Row {i+1} ({row.get("Nom_Classe","?")}): empty value in column {col}')

    # Check no duplicate class names
    names = [row['Nom_Classe'] for row in csv_rows]
    dupes = [n for n in names if names.count(n) > 1]
    if dupes:
        errors.append(f'Duplicate class names: {set(dupes)}')

    # Check numeric columns are valid integers
    for i, row in enumerate(csv_rows):
        for col in ['Nb_Methodes', 'Nb_Attributs', 'Lignes_de_Code']:
            try:
                int(row[col])
            except (ValueError, KeyError):
                errors.append(f'Row {i+1} ({row.get("Nom_Classe","?")}): non-integer value in {col}: {row.get(col)}')

    return errors


# ============================================================
# Level 3: Logical invariants
# ============================================================
def validate_invariants(csv_rows, json_metrics):
    errors = []

    for row in csv_rows:
        name = row['Nom_Classe']
        nb_methods = int(row['Nb_Methodes'])
        nb_attrs = int(row['Nb_Attributs'])
        loc = int(row['Lignes_de_Code'])

        # No negative values
        if nb_methods < 0:
            errors.append(f'{name}: Nb_Methodes is negative ({nb_methods})')
        if nb_attrs < 0:
            errors.append(f'{name}: Nb_Attributs is negative ({nb_attrs})')
        if loc < 0:
            errors.append(f'{name}: Lignes_de_Code is negative ({loc})')

        # LOC should not be 0 if there are methods
        if nb_methods > 0 and loc == 0:
            errors.append(f'{name}: has {nb_methods} methods but LOC=0 (likely a computation error)')

        # LOC should be >= number of methods (at minimum 1 line per method)
        if loc > 0 and loc < nb_methods:
            errors.append(f'{name}: LOC ({loc}) < Nb_Methodes ({nb_methods}) - suspicious')

    # Check at least 1 class was exported
    if len(csv_rows) == 0:
        errors.append('No classes exported')

    return errors


# ============================================================
# Level 2+4: Cross-verification (Python vs Pharo)
# ============================================================
def validate_cross_check(csv_rows, json_metrics):
    errors = []

    csv_classes = {row['Nom_Classe'] for row in csv_rows}
    json_classes = set(json_metrics.keys())

    # Check same set of classes
    missing_in_csv = json_classes - csv_classes
    extra_in_csv = csv_classes - json_classes
    if missing_in_csv:
        errors.append(f'Classes in JSON but missing from CSV: {missing_in_csv}')
    if extra_in_csv:
        errors.append(f'Classes in CSV but missing from JSON: {extra_in_csv}')

    # Compare metric values
    for row in csv_rows:
        name = row['Nom_Classe']
        if name not in json_metrics:
            continue

        expected = json_metrics[name]
        csv_methods = int(row['Nb_Methodes'])
        csv_attrs = int(row['Nb_Attributs'])
        csv_loc = int(row['Lignes_de_Code'])

        if csv_methods != expected['Nb_Methodes']:
            errors.append(
                f'{name}: Nb_Methodes mismatch — CSV={csv_methods}, Python={expected["Nb_Methodes"]}'
            )

        if csv_attrs != expected['Nb_Attributs']:
            errors.append(
                f'{name}: Nb_Attributs mismatch — CSV={csv_attrs}, Python={expected["Nb_Attributs"]}'
            )

        loc_diff = abs(csv_loc - expected['Lignes_de_Code'])
        if loc_diff > LOC_TOLERANCE:
            errors.append(
                f'{name}: Lignes_de_Code mismatch — CSV={csv_loc}, Python={expected["Lignes_de_Code"]} (diff={loc_diff})'
            )
        elif loc_diff > 0:
            print(f'  [INFO] {name}: minor LOC difference (CSV={csv_loc}, Python={expected["Lignes_de_Code"]}) — within tolerance')

    return errors


# ============================================================
# Main
# ============================================================
def main():
    csv_path = sys.argv[1] if len(sys.argv) > 1 else 'export_metrics.csv'
    json_path = sys.argv[2] if len(sys.argv) > 2 else 'model.json'
    source_root = sys.argv[3] if len(sys.argv) > 3 else '.'

    print('=' * 60)
    print('VALIDATION DES METRIQUES DU PIPELINE')
    print('=' * 60)
    print(f'CSV:  {csv_path}')
    print(f'JSON: {json_path}')
    print(f'Source root: {source_root}')
    print()

    all_errors = []

    # Load CSV
    print('[Level 1] Validation structurelle du CSV...')
    csv_rows = load_csv(csv_path)
    print(f'  Classes dans le CSV: {len(csv_rows)}')
    l1_errors = validate_structure(csv_rows, csv_path)
    all_errors.extend(l1_errors)
    if l1_errors:
        for e in l1_errors:
            print(f'  ERREUR: {e}')
    else:
        print('  OK')
    print()

    # Compute independent metrics from JSON
    print('[Level 2+4] Calcul independant des metriques depuis le JSON Famix...')
    data = load_famix_json(json_path)
    json_metrics = compute_metrics_from_json(data, source_root)
    print(f'  Classes dans le JSON: {len(json_metrics)}')
    for name, m in sorted(json_metrics.items()):
        print(f'    {name}: methods={m["Nb_Methodes"]}, attrs={m["Nb_Attributs"]}, loc={m["Lignes_de_Code"]}')
    print()

    # Invariants
    print('[Level 3] Verification des invariants logiques...')
    l3_errors = validate_invariants(csv_rows, json_metrics)
    all_errors.extend(l3_errors)
    if l3_errors:
        for e in l3_errors:
            print(f'  ERREUR: {e}')
    else:
        print('  OK')
    print()

    # Cross-check
    print('[Level 2+4] Verification croisee Pharo vs Python...')
    l24_errors = validate_cross_check(csv_rows, json_metrics)
    all_errors.extend(l24_errors)
    if l24_errors:
        for e in l24_errors:
            print(f'  ERREUR: {e}')
    else:
        print('  OK — les metriques Pharo et Python correspondent')
    print()

    # Summary
    print('=' * 60)
    if all_errors:
        print(f'ECHEC: {len(all_errors)} erreur(s) detectee(s)')
        for e in all_errors:
            print(f'  - {e}')
        print('=' * 60)
        sys.exit(1)
    else:
        print('SUCCES: toutes les validations sont passees')
        print('=' * 60)
        sys.exit(0)


if __name__ == '__main__':
    main()
